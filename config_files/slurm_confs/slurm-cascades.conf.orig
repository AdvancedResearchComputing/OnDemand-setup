#Ansible managed
########################################################
#  Configuration file for SLURM                        #
########################################################
#
#
#
################################################
#                   CONTROL                    #
################################################
#BackupAddr=
BackupController=ca-slurmctld-2.ca.arc.internal
ClusterName=cascades
#ControlAddr=
ControlMachine=ca-slurmctld-1.ca.arc.internal
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
SlurmdUser=root
#
################################################
#            LOGGING & OTHER PATHS             #
################################################
SlurmctldLogFile=/var/log/slurm/ctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdPidFile=/var/run/slurmd.pid
SlurmdSpoolDir=/var/spool/slurmd
SlurmSchedLogFile=/var/log/slurm/scheduler.log
SlurmctldPidFile=/var/run/slurmctld.pid
StateSaveLocation=/caslurmstate
#
################################################
#                  ACCOUNTING                  #
################################################
AccountingStorageBackupHost=slurmdbd-2.arc.internal
AccountingStorageEnforce=associations,limits,safe,qos
AccountingStorageHost=slurmdbd-1.arc.internal
#AccountingStorageLoc=
AccountingStoragePort=6819
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
AccountingStoreJobComment=Yes
AcctGatherEnergyType=acct_gather_energy/ipmi
AcctGatherFilesystemType=acct_gather_filesystem/none
AcctGatherInterconnectType=acct_gather_interconnect/none
AcctGatherNodeFreq=0
AcctGatherProfileType=acct_gather_profile/hdf5
ExtSensorsType=ext_sensors/none
ExtSensorsFreq=0
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/cgroup
#
################################################
#           SCHEDULING & ALLOCATION            #
################################################
FastSchedule=1
PreemptMode=REQUEUE
PreemptType=preempt/partition_prio
PriorityType=priority/multifactor
PriorityDecayHalfLife=0
PriorityUsageResetPeriod=WEEKLY
#SchedulerParameters=
SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
SlurmSchedLogLevel=1
#
################################################
#                   TOPOLOGY                   #
################################################
TopologyPlugin=topology/none
#
################################################
#                    TIMERS                    #
################################################
BatchStartTimeout=10
CompleteWait=0
EpilogMsgTime=2000
GetEnvTimeout=2
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
WaitTime=0
#
################################################
#                    POWER                     #
################################################
#ResumeProgram=
ResumeRate=300
ResumeTimeout=60
#SuspendExcNodes=
#SuspendExcParts=
#SuspendProgram=
SuspendRate=60
SuspendTime=NONE
SuspendTimeout=30
#
################################################
#                    DEBUG                     #
################################################
#DebugFlags=
SlurmctldDebug=info
SlurmdDebug=info
#
################################################
#               EPILOG & PROLOG                #
################################################
Epilog=/usr/local/bin/epilog
Prolog=/usr/local/bin/prolog
#SrunEpilog=
#SrunProlog=
#TaskEpilog=
TaskProlog=/usr/local/bin/task_prolog

################################################
#               PROCESS TRACKING               #
################################################
ProctrackType=proctrack/cgroup
#
################################################
#             RESOURCE CONFINEMENT             #
################################################
TaskPlugin=task/affinity,task/cgroup
#TaskPluginParam=
#
################################################
#                    OTHER                     #
################################################
AccountingStorageTRES=cpu,mem,energy,node,billing,gres/gpu
AllowSpecResourcesUsage=0
#AuthInfo=
AuthType=auth/munge
#BurstBufferType=
CheckpointType=checkpoint/none
#ChosLoc=
CoreSpecPlugin=core_spec/none
CpuFreqDef=Performance
CpuFreqGovernors=Performance,OnDemand
CryptoType=crypto/munge
#DefMemPerNode=
DisableRootJobs=No
EioTimeout=60
EnforcePartLimits=NO
#EpilogSlurmctld=
#FederationParameters=
FirstJobId=1
GresTypes=gpu
GroupUpdateForce=1
GroupUpdateTime=600
HealthCheckInterval=0
HealthCheckNodeState=ANY
#HealthCheckProgram=
#JobAcctGatherParams=
JobCheckpointDir=/var/slurm/checkpoint
JobCompHost=localhost
JobCompLoc=/var/log/openondemand_jobcomp.log
JobCompPort=0
JobCompType=jobcomp/none
JobCompUser=root
JobContainerType=job_container/none
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
JobFileAppend=0
JobRequeue=1
#JobSubmitPlugins=
#KeepAliveTime=
KillOnBadExit=0
#LaunchParameters=
LaunchType=launch/slurm
#Layouts=
#Licenses=
#LicensesUsed=
LogTimeFormat=iso8601_ms
#MailDomain=
MailProg=/bin/mail
MaxArraySize=1001
MaxJobCount=10000
MaxJobId=67043328
#MaxMemPerNode=UNLIMITED
DefMemPerCPU=8196
MaxMemPerCPU=10240
MaxStepCount=40000
MaxTasksPerNode=512
MCSPlugin=mcs/none
#MCSParameters=
MemLimitEnforce=Yes
MessageTimeout=10
MpiDefault=none
#MpiParams=
#MsgAggregationParams=
#NodeFeaturesPlugins=
OverTimeLimit=0
PluginDir=/usr/lib64/slurm
PlugStackConfig=/etc/slurm/plugstack.conf
#PowerParameters=
#PowerPlugin=
#PriorityParameters=
PrivateData=none
PrologEpilogTimeout=65534
#PrologSlurmctld=
PrologFlags=Contain
PropagatePrioProcess=0
PropagateResourceLimits=ALL
#PropagateResourceLimitsExcept=
RebootProgram="/usr/sbin/shutdown -r now"
#ReconfigFlags=
#RequeueExit=
#RequeueExitHold=
#ResvEpilog=
ResvOverRun=0
#ResvProlog=
ReturnToService=1
RoutePlugin=route/default
SallocDefaultCommand="srun -n1 -N1 --mem-per-cpu=0 --pty --preserve-env --cpu-bind=no --mpi=none $SHELL"
#SbcastParameters=
SlurmctldSyslogDebug=quiet
SlurmdSyslogDebug=quiet
#SlurmctldPlugstack=
SrunPortRange=0-0
SwitchType=switch/none
TCPTimeout=2
TmpFS=/scratch-local
#TopologyParam=
TrackWCKey=No
TreeWidth=50
UsePam=1
#UnkillableStepProgram=
UnkillableStepTimeout=60
VSizeFactor=0
#
################################################
#                    NODES                     #
################################################

NodeName=DEFAULT Boards=1 ThreadsPerCore=1 State=UNKNOWN
NodeName=ca[001-002] RealMemory=3096017 CPUs=72 SocketsPerBoard=4 CoresPerSocket=18 Weight=45
NodeName=ca[003-006] RealMemory=515694 CPUs=32 SocketsPerBoard=2 CoresPerSocket=16 Gres=gpu:k80:4 Weight=32
NodeName=ca[007-196] RealMemory=128646 CPUs=32 SocketsPerBoard=2 CoresPerSocket=16 Weight=16
NodeName=ca[197-236] RealMemory=385300 CPUs=24 SocketsPerBoard=2 CoresPerSocket=12 Gres=gpu:v100:2 Weight=40

#
################################################
#                  PARTITIONS                  #
################################################
#Defaults for all partitions
PartitionName=DEFAULT AllowGroups=ALL AllowAccounts=ALL Default=NO DefaultTime=00:30:00 Hidden=NO DisableRootJobs=NO MinNodes=1 RootOnly=NO OverSubscribe=NO PreemptMode=OFF State=UP MaxMemPerNode=UNLIMITED PriorityTier=16

#Defaults for standard usage partitions (normal, open, dev)
PartitionName=DEFAULT MaxNodes=32 Nodes=ca[007-196] DefMemPerCPU=3700
PartitionName=dev_q MaxTime=0-02:00:00 qos=ca_dev_q
PartitionName=normal_q MaxTime=6-00:00:00 qos=ca_normal_q
PartitionName=open_q Default=YES MaxNodes=4 MaxTime=0-04:00:00 qos=ca_open_q
PartitionName=preemptable_q MaxNodes=64 MaxTime=0-04:00:00 PreemptMode=REQUEUE PriorityTier=8 qos=ca_preemptable_q
PartitionName=srinivasan_lab Hidden=YES AllowGroups=admin,arcadm,srinivasan_AOE_lab AllowAccounts=arctest,plasim MaxNodes=64 MaxTime=7-00:00:00 qos=ca_srinivasan_AOE_lab
PartitionName=jspitt_lab Hidden=YES AllowGroups=admin,arcadm,jspitt_lab MaxNodes=64 MaxTime=7-00:00:00 qos=ca_jspitt_lab
PartitionName=large_q Hidden=YES AllowGroups=admin,arcadm,cascades_large_q MaxNodes=UNLIMITED MaxTime=0-02:00:00 qos=ca_large_q
PartitionName=long_q Hidden=YES AllowGroups=admin,arcadm,cascades_long_q MaxNodes=UNLIMITED MaxTime=6-00:00:00 qos=ca_long_q

#Other HW partitions
PartitionName=largemem_q MaxNodes=1 MaxTime=6-00:00:00  Nodes=ca[001-002] DefMemPerCPU=42858 TRESBillingWeights="CPU=1.0" qos=ca_largemem_q
PartitionName=k80_q  MaxNodes=4 MaxTime=6-00:00:00  Nodes=ca[003-006] DefMemPerCPU=15795 TRESBillingWeights="CPU=1.0" qos=ca_k80_q

#Default mods for v100 partitions
PartitionName=DEFAULT Nodes=ca[197-236] DefMemPerCPU=15627
PartitionName=v100_normal_q  MaxNodes=12 MaxTime=6-00:00:00  TRESBillingWeights="CPU=1.0" qos=ca_v100_normal_q
PartitionName=v100_large_q AllowGroups=admin,arcadm,testing,v100_large_q AllowAccounts=arctest Hidden=YES MaxNodes=UNLIMITED MaxTime=6-00:00:00 TRESBillingWeights="CPU=1.0" qos=ca_v100_large_q
PartitionName=v100_dev_q MaxNodes=8 MaxTime=0-02:00:00 TRESBillingWeights="CPU=1.0" qos=ca_v100_dev_q

